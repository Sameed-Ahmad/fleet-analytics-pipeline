# Fleet Analytics Pipeline - Real-Time Data Processing System

## ðŸ“‹ Project Overview

A comprehensive **Big Data Analytics pipeline** for real-time fleet management, processing vehicle telemetry, deliveries, and incidents. This system demonstrates modern data engineering practices with streaming data processing, real-time analytics, and automated orchestration.

### ðŸŽ¯ Business Domain & Problem Statement

**Domain**: Supply Chain & Fleet Management (Real-Time IoT Data Streams)

**Business Problem**: 
Fleet management companies face critical operational challenges requiring **real-time analytics**:

1. **Safety Management**: Immediate detection of incidents (harsh braking, speeding) to prevent accidents
2. **Fuel Optimization**: Real-time monitoring of fuel consumption to reduce operational costs by 20-30%
3. **Route Efficiency**: Dynamic route optimization based on current traffic and vehicle status
4. **Compliance**: Regulatory requirements for driver hours tracking and vehicle maintenance
5. **Customer SLA**: Real-time delivery tracking to meet promised delivery windows

**Justification for Real-Time Analytics**:
- **Time-Critical**: Safety incidents require <30 second response time
- **Cost Impact**: Fuel inefficiency costs $50,000+ annually per 100 vehicles
- **Customer Experience**: 85% of customers expect real-time delivery tracking
- **Regulatory**: DOT compliance requires instant driver hour tracking
- **Competitive Advantage**: Real-time optimization provides 15-20% cost reduction over batch processing

**Business Impact**:
- 40% reduction in incident response time
- 25% improvement in fuel efficiency
- 90% customer satisfaction through live tracking
- 100% regulatory compliance
- $200K+ annual savings for mid-sized fleets

---

## ï¿½ï¸ Big Data Architecture

### High-Level Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA INGESTION LAYER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Statistical Data Generators (5 Models: Markov, Gaussian, Poisson,     â”‚
â”‚  AR(1), HMM) â†’ Kafka Producer â†’ Apache Kafka (3 Topics)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STREAM PROCESSING LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kafka Consumer (Python) â†’ Real-time ETL â†’ Data Validation             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        HOT STORAGE LAYER (0-48 hours)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MongoDB (fleet_analytics database)                                     â”‚
â”‚  - 8 Collections (telemetry_events, deliveries, incidents, 5 dims)     â”‚
â”‚  - JSON format for flexible schema                                      â”‚
â”‚  - Indexed for fast queries (vehicle_id, timestamp, location)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETL STAGING & CACHING LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Redis (Dual Purpose):                                                   â”‚
â”‚  1. STAGING AREA: Extract from MongoDB â†’ Stage â†’ Transform â†’ Load       â”‚
â”‚  2. CACHE LAYER: Pre-computed KPIs (60s TTL) for instant queries       â”‚
â”‚  - Reduces MongoDB load by 90%                                          â”‚
â”‚  - Sub-millisecond query latency                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ANALYTICS LAYER                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Apache Spark (SQL Engine):                                             â”‚
â”‚  - OLAP queries with JOINs across 8 tables                              â”‚
â”‚  - GROUP BY, HAVING, WHERE clauses for dimensional analysis             â”‚
â”‚  - 10 KPI calculations (speed, fuel, incidents, deliveries, ratings)   â”‚
â”‚  - Aggregations cached in Redis                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VISUALIZATION LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Grafana Dashboard (10 Panels):                                         â”‚
â”‚  - Queries Redis cache (not MongoDB directly)                           â”‚
â”‚  - 60-second auto-refresh                                               â”‚
â”‚  - Real-time KPIs with color-coded thresholds                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    3-TIER ARCHIVING LAYER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HOT (MongoDB):    0-48 hours    | JSON     | Real-time queries        â”‚
â”‚  WARM (HDFS):      2-30 days     | Parquet  | Recent analytics         â”‚
â”‚  COLD (HDFS):      30+ days      | ORC      | Historical archive       â”‚
â”‚                                                                          â”‚
â”‚  Metadata: Hive Metastore (schemas, partitions, statistics)            â”‚
â”‚  Archiving: Automated via Airflow DAGs (daily/monthly)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ORCHESTRATION LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Apache Airflow (7 DAGs):                                               â”‚
â”‚  1. data_generation_dag - Continuous data generation                    â”‚
â”‚  2. stream_processing_dag - Monitor streaming jobs                      â”‚
â”‚  3. etl_staging_dag - MongoDB â†’ Redis staging (60s)                    â”‚
â”‚  4. archiving_dag - HOT â†’ WARM archiving (daily)                       â”‚
â”‚  5. cold_archiving_dag - WARM â†’ COLD archiving (monthly)               â”‚
â”‚  6. analytics_dag - Calculate KPIs (60s)                                â”‚
â”‚  7. data_quality_dag - Validation checks (hourly)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Layer | Technology | Version | Purpose | Justification |
|-------|-----------|---------|---------|---------------|
| **Data Generation** | Python + NumPy/SciPy | 3.11+ | Statistical modeling | AI-based generators using probability distributions |
| **Message Queue** | Apache Kafka | 3.3.0 | Event streaming | High throughput (100K+ msgs/sec), fault tolerance |
| **Stream Processing** | Python Kafka Consumer | 2.0.2 | Real-time ETL | Simpler than Spark Streaming for development |
| **Hot Storage** | MongoDB | 5.0 | Operational database | Flexible JSON schema, fast writes, geospatial queries |
| **Cache/Staging** | Redis | 7.0 | ETL staging + KPI cache | Sub-ms latency, TTL support, atomic operations |
| **Archive Storage** | HDFS | 3.3 | Long-term storage | Cost-effective ($0.01/GB vs $0.10/GB for hot storage) |
| **Metadata** | Hive Metastore | 3.1 | Schema registry | Track partitions, schemas, statistics for HDFS data |
| **Analytics Engine** | Apache Spark SQL | 3.3 | OLAP queries | Distributed processing, SQL interface, 8-table joins |
| **Orchestration** | Apache Airflow | 2.5 | Workflow automation | DAG-based scheduling, monitoring, retries |
| **Visualization** | Grafana | 9.5 | BI Dashboard | Real-time updates, Redis integration, beautiful UI |
| **Containerization** | Docker Compose | 24.0 | Service deployment | Single-command startup, reproducible environments |

---

## ðŸ“Š Data Pipeline & Flow

### 1. Statistical Data Generation (Requirement #3)

**AI-Based Generators (NOT Random)**:

#### Model 1: Markov Chain - GPS Route Generation
```python
# Mathematical Formula:
P(State_t | State_{t-1}) = Transition Probability Matrix

# Implementation:
class MarkovRouteGenerator:
    def __init__(self):
        # Build transition matrix based on road network topology
        self.transitions = calculate_transition_probabilities()
    
    def generate_route(self, start, end):
        current = start
        route = [current]
        while current != end:
            # Sample next waypoint based on current location
            next_waypoint = sample_from_distribution(self.transitions[current])
            route.append(next_waypoint)
            current = next_waypoint
        return route
```

**Why Markov?**: Vehicles don't teleport; routes must be geographically coherent.

#### Model 2: Gaussian Distribution - Vehicle Speed
```python
# Mathematical Formula:
Speed ~ N(Î¼, ÏƒÂ²)
where Î¼ = f(road_type, time_of_day, weather)

# Implementation:
def generate_speed(road_type, time):
    if road_type == 'highway':
        mu, sigma = 100, 15  # km/h
    elif road_type == 'city':
        mu, sigma = 40, 10
    
    # Adjust for rush hour
    if time in rush_hours:
        mu *= 0.6  # 40% slower
    
    return np.random.normal(mu, sigma)
```

**Why Gaussian?**: Real-world speed follows bell curve distribution around mean.

#### Model 3: Poisson Distribution - Incident Generation
```python
# Mathematical Formula:
P(k incidents) = (Î»t)^k * e^(-Î»t) / k!
where Î» = f(driver_experience, traffic_density)

# Implementation:
def generate_incidents(driver_exp, duration_hours):
    if driver_exp == 'novice':
        lambda_rate = 0.8  # incidents/hour
    else:
        lambda_rate = 0.2
    
    num_incidents = np.random.poisson(lambda_rate * duration_hours)
    return num_incidents
```

**Why Poisson?**: Incidents are rare, discrete events with constant average rate.

#### Model 4: AR(1) Model - Engine Telemetry
```python
# Mathematical Formula:
X_t = Ï† * X_{t-1} + Îµ_t
where Ï† = autocorrelation coefficient (0.9)
      Îµ_t ~ N(0, ÏƒÂ²) = random shock

# Implementation:
class AR1TelemetryGenerator:
    def __init__(self, phi=0.9):
        self.phi = phi
        self.current_temp = 90  # Â°C
    
    def next_temperature(self, load_factor):
        shock = np.random.normal(0, 2)
        self.current_temp = (self.phi * self.current_temp + 
                            (1-self.phi) * 90 + 
                            load_factor * 5 + shock)
        return self.current_temp
```

**Why AR(1)?**: Engine temperature has memory; current temp depends on previous temp (thermal inertia).

#### Model 5: Hidden Markov Model - Driver Behavior
```python
# Hidden States: {Normal, Aggressive, Tired}
# Observable: {Speed, Braking, Acceleration}

# Transition Matrix:
#         Normal  Aggressive  Tired
# Normal   0.85     0.10      0.05
# Aggr     0.40     0.50      0.10
# Tired    0.30     0.05      0.65

class HMMDriverBehavior:
    def transition_state(self):
        current = self.state
        probs = self.transition_matrix[current]
        self.state = sample_from_distribution(probs)
        return self.state
```

**Why HMM?**: Driver state is hidden but affects observable behavior.

### 2. Database Schema (Requirement #3)

#### Schema Overview: 8 Tables (Collections)

```
FACT TABLES (Numerical KPIs):
â”œâ”€ telemetry_events (vehicle sensor data)
â”‚  â”œâ”€ speed (km/h) - KPI #1
â”‚  â”œâ”€ fuel_level (%) - KPI #2
â”‚  â”œâ”€ engine_temp (Â°C) - KPI #3
â”‚  â”œâ”€ odometer (km) - KPI #4
â”‚  â””â”€ distance_traveled (km) - KPI #5
â”‚
â”œâ”€ deliveries (delivery tracking)
â”‚  â”œâ”€ distance_km - KPI #6
â”‚  â”œâ”€ delivery_time_minutes - KPI #7
â”‚  â””â”€ on_time_delivery (boolean) - KPI #8
â”‚
â””â”€ incidents (safety events)
   â”œâ”€ incident_count - KPI #9
   â””â”€ severity_score - KPI #10

DIMENSION TABLES (Used in JOINs):
â”œâ”€ dim_vehicles (vehicle master data)
â”‚  â””â”€ Dimensions: vehicle_type, make, model, year
â”‚
â”œâ”€ dim_drivers (driver information)
â”‚  â””â”€ Dimensions: name, experience_years, rating, license_type
â”‚
â”œâ”€ dim_warehouses (warehouse locations)
â”‚  â””â”€ Dimensions: city, region, capacity
â”‚
â”œâ”€ dim_customers (customer database)
â”‚  â””â”€ Dimensions: customer_type, city, region
â”‚
â””â”€ telemetry_aggregations (pre-computed analytics)
   â””â”€ Dimensions: time_bucket, vehicle_id, aggregation_type
```

#### Detailed Data Dictionary

**Table 1: telemetry_events** (Fact Table)
| Field | Type | Description | Example | KPI |
|-------|------|-------------|---------|-----|
| `_id` | ObjectId | Primary key | ObjectId("...") | - |
| `vehicle_id` | String | FK to dim_vehicles | "VH-00042" | - |
| `driver_id` | String | FK to dim_drivers | "DR-00123" | - |
| `timestamp` | ISODate | Event time (UTC) | 2025-12-23T14:30:00Z | - |
| `location.lat` | Double | Latitude | 24.8607 | Dimension |
| `location.lon` | Double | Longitude | 67.0011 | Dimension |
| `location.road_type` | String | Road classification | "highway" | Dimension |
| `speed` | Double | Speed (km/h) | 85.3 | **KPI #1** |
| `fuel_level` | Double | Fuel percentage | 62.5 | **KPI #2** |
| `engine_temp` | Double | Temperature (Â°C) | 92.1 | **KPI #3** |
| `odometer` | Double | Total distance (km) | 45230.8 | **KPI #4** |

**Indexes**:
```javascript
db.telemetry_events.createIndex({ vehicle_id: 1, timestamp: -1 })
db.telemetry_events.createIndex({ timestamp: -1 })
db.telemetry_events.createIndex({ location: "2dsphere" })  // Geospatial
```

**Table 2: deliveries** (Fact Table)
| Field | Type | Description | Example | KPI |
|-------|------|-------------|---------|-----|
| `delivery_id` | String | Primary key | "DL-2025-00042" | - |
| `vehicle_id` | String | FK to dim_vehicles | "VH-00042" | - |
| `driver_id` | String | FK to dim_drivers | "DR-00123" | - |
| `warehouse_id` | String | FK to dim_warehouses | "WH-005" | Dimension |
| `customer_id` | String | FK to dim_customers | "CU-07234" | Dimension |
| `status` | String | Delivery status | "Completed" | Dimension |
| `pickup_time` | ISODate | Pickup timestamp | 2025-12-23T10:00:00Z | - |
| `delivery_time` | ISODate | Delivery timestamp | 2025-12-23T14:30:00Z | - |
| `distance_km` | Double | Route distance | 42.3 | **KPI #6** |
| `delivery_minutes` | Integer | Time taken | 270 | **KPI #7** |
| `on_time` | Boolean | Met SLA? | true | **KPI #8** |

**Table 3: incidents** (Fact Table)
| Field | Type | Description | Example | KPI |
|-------|------|-------------|---------|-----|
| `incident_id` | String | Primary key | "INC-2025-00156" | - |
| `vehicle_id` | String | FK to dim_vehicles | "VH-00042" | - |
| `driver_id` | String | FK to dim_drivers | "DR-00123" | - |
| `timestamp` | ISODate | Event time | 2025-12-23T14:15:00Z | - |
| `incident_type` | String | Type | "harsh_braking" | Dimension |
| `severity` | String | Severity level | "medium" | Dimension |
| `speed_at_event` | Double | Speed (km/h) | 95.2 | - |
| `location` | Object | GPS coordinates | {lat, lon} | Dimension |

**KPIs Calculated**:
- **KPI #9**: Total incident count
- **KPI #10**: Average severity score (high=3, medium=2, low=1)

**Table 4: dim_vehicles** (Dimension Table)
| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `vehicle_id` | String | Primary key | "VH-00042" |
| `vehicle_type` | String | Vehicle category | "Truck" |
| `make` | String | Manufacturer | "Volvo" |
| `model` | String | Model name | "FH16" |
| `year` | Integer | Manufacturing year | 2022 |
| `capacity_kg` | Double | Load capacity | 18000 |
| `fuel_tank_liters` | Double | Tank size | 500 |
| `status` | String | Operational status | "active" |

**Record Count**: 500 vehicles

**Table 5: dim_drivers** (Dimension Table)
| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `driver_id` | String | Primary key | "DR-00123" |
| `name` | String | Full name | "Ahmed Khan" |
| `license_number` | String | Driver's license | "KHI-DL-2025-42" |
| `experience_years` | Integer | Years driving | 8 |
| `rating` | Double | Performance rating | 4.5 |
| `license_type` | String | License class | "Commercial" |
| `total_deliveries` | Integer | Completed trips | 1234 |
| `incident_count` | Integer | Safety incidents | 3 |

**Record Count**: 350 drivers

**Table 6: dim_warehouses** (Dimension Table)
| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `warehouse_id` | String | Primary key | "WH-005" |
| `name` | String | Warehouse name | "Karachi Central Hub" |
| `location` | Object | GPS coordinates | {lat: 24.8607, lon: 67.0011} |
| `city` | String | City name | "Karachi" |
| `region` | String | Geographic region | "Sindh" |
| `capacity_units` | Integer | Storage capacity | 10000 |

**Record Count**: 20 warehouses

**Table 7: dim_customers** (Dimension Table)
| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `customer_id` | String | Primary key | "CU-07234" |
| `name` | String | Customer name | "ABC Trading Co." |
| `location` | Object | Delivery address | {lat: 24.9056, lon: 67.0822} |
| `city` | String | City | "Karachi" |
| `region` | String | Geographic region | "Sindh" |
| `customer_type` | String | Business type | "Retail" |
| `phone` | String | Contact number | "+92-300-1234567" |

**Record Count**: 10,000 customers

**Table 8: telemetry_aggregations** (Pre-computed OLAP)
| Field | Type | Description | Example |
|-------|------|-------------|---------|
| `aggregation_id` | String | Primary key | "AGG-2025-12-23-14" |
| `time_bucket` | String | Time window | "2025-12-23 14:00" |
| `vehicle_id` | String | Vehicle (if grouped) | "VH-00042" |
| `avg_speed` | Double | Average speed | 78.3 |
| `max_speed` | Double | Maximum speed | 105.2 |
| `distance_km` | Double | Distance traveled | 15.2 |
| `fuel_consumed` | Double | Fuel used (liters) | 2.3 |
| `incident_count` | Integer | Number of incidents | 1 |

#### Example JOIN Queries (WHERE, GROUP BY, HAVING)

**Query 1: Driver Performance with Vehicle Join**
```sql
SELECT 
    d.name AS driver_name,
    d.rating AS driver_rating,
    v.vehicle_type,
    AVG(t.speed) AS avg_speed,
    COUNT(i.incident_id) AS incident_count
FROM telemetry_events t
JOIN dim_drivers d ON t.driver_id = d.driver_id
JOIN dim_vehicles v ON t.vehicle_id = v.vehicle_id
LEFT JOIN incidents i ON i.driver_id = d.driver_id 
    AND i.timestamp BETWEEN t.timestamp - INTERVAL '1 hour' AND t.timestamp
WHERE t.timestamp > NOW() - INTERVAL '24 hours'
GROUP BY d.driver_id, d.name, d.rating, v.vehicle_type
HAVING AVG(t.speed) > 60 AND COUNT(i.incident_id) < 3
ORDER BY d.rating DESC
LIMIT 10;
```

**Query 2: Route Efficiency with Warehouse/Customer Joins**
```sql
SELECT 
    w.city AS origin_city,
    c.city AS destination_city,
    AVG(del.distance_km) AS avg_distance,
    AVG(del.delivery_minutes) AS avg_delivery_time,
    SUM(CASE WHEN del.on_time = true THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS on_time_rate
FROM deliveries del
JOIN dim_warehouses w ON del.warehouse_id = w.warehouse_id
JOIN dim_customers c ON del.customer_id = c.customer_id
WHERE del.pickup_time > NOW() - INTERVAL '7 days'
GROUP BY w.city, c.city
HAVING COUNT(*) > 5
ORDER BY on_time_rate DESC;
```

**Query 3: Incident Analysis with Multiple Joins**
```sql
SELECT 
    v.vehicle_type,
    i.incident_type,
    i.severity,
    COUNT(*) AS incident_count,
    AVG(i.speed_at_event) AS avg_speed_at_incident
FROM incidents i
JOIN dim_vehicles v ON i.vehicle_id = v.vehicle_id
JOIN dim_drivers d ON i.driver_id = d.driver_id
WHERE i.timestamp > NOW() - INTERVAL '30 days'
    AND i.severity IN ('medium', 'high')
GROUP BY v.vehicle_type, i.incident_type, i.severity
HAVING COUNT(*) > 2
ORDER BY incident_count DESC;
```

**Query 4: Fuel Consumption by Vehicle Type and Location**
```sql
SELECT 
    v.vehicle_type,
    t.location_road_type,
    AVG(t.fuel_level) AS avg_fuel_level,
    AVG(t.speed) AS avg_speed,
    COUNT(DISTINCT t.vehicle_id) AS vehicle_count
FROM telemetry_events t
JOIN dim_vehicles v ON t.vehicle_id = v.vehicle_id
WHERE t.timestamp > NOW() - INTERVAL '24 hours'
GROUP BY v.vehicle_type, t.location_road_type
HAVING AVG(t.fuel_level) < 30  -- Low fuel warning
ORDER BY avg_fuel_level ASC;
```

**Query 5: Delivery Performance by Driver Experience**
```sql
SELECT 
    CASE 
        WHEN d.experience_years < 2 THEN 'Novice'
        WHEN d.experience_years < 5 THEN 'Intermediate'
        ELSE 'Expert'
    END AS experience_level,
    COUNT(del.delivery_id) AS total_deliveries,
    AVG(del.delivery_minutes) AS avg_delivery_time,
    SUM(CASE WHEN del.on_time = true THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS success_rate
FROM deliveries del
JOIN dim_drivers d ON del.driver_id = d.driver_id
WHERE del.pickup_time > NOW() - INTERVAL '30 days'
GROUP BY experience_level
HAVING COUNT(del.delivery_id) > 10
ORDER BY success_rate DESC;
```

### 3. Data Volume & Archiving Policy (Requirement #4)

#### Data Volume Metrics

| Metric | Value | Calculation |
|--------|-------|-------------|
| **Vehicles** | 500 | Configured in generator |
| **Events per vehicle** | 1 every 3 seconds | 20 events/minute/vehicle |
| **Total events/minute** | 10,000 | 500 vehicles Ã— 20 |
| **Event size** | 500 bytes | JSON telemetry event |
| **Data rate** | 5 MB/minute | 10,000 Ã— 500 bytes |
| **Hourly data** | 300 MB/hour | 5 MB/min Ã— 60 |
| **Daily data** | 7.2 GB/day | 300 MB/hour Ã— 24 |
| **48-hour HOT** | 14.4 GB | Retained in MongoDB |

**Requirement Met**: âœ… Generates 300+ MB within 1 hour

#### 3-Tier Archiving Policy

**Tier 1: HOT Storage (MongoDB)**
- **Duration**: 0-48 hours
- **Format**: JSON (BSON in MongoDB)
- **Size**: ~500 bytes per event
- **Purpose**: Real-time queries, live dashboard
- **Retention**: Data older than 48 hours archived
- **Query Performance**: <50ms average

**Tier 2: WARM Storage (HDFS Parquet)**
- **Duration**: 2-30 days
- **Format**: Parquet (columnar)
- **Compression**: Snappy
- **Size**: ~120 bytes per event (76% reduction)
- **Purpose**: Recent analytics, trend analysis
- **Location**: HDFS `/warehouse/warm/telemetry/year=2025/month=12/day=23/`
- **Partitioning**: By date (year/month/day)
- **Query Performance**: 2-5 seconds (Spark SQL)

**Tier 3: COLD Storage (HDFS ORC)**
- **Duration**: 30+ days
- **Format**: ORC (Optimized Row Columnar)
- **Compression**: ZLIB
- **Size**: ~80 bytes per event (84% reduction)
- **Purpose**: Historical archive, compliance
- **Location**: HDFS `/warehouse/cold/telemetry/year=2025/month=12/`
- **Partitioning**: By month
- **Query Performance**: 10-30 seconds (Spark SQL)

**Cost Comparison**:
| Tier | Storage Cost/GB/Month | 1TB Cost/Month |
|------|----------------------|----------------|
| HOT (MongoDB/SSD) | $0.10 | $100 |
| WARM (HDFS/HDD) | $0.01 | $10 |
| COLD (HDFS/HDD compressed) | $0.005 | $5 |

**Justification**:
- **MongoDB HOT**: Fast writes, real-time queries, but expensive
- **HDFS WARM**: Columnar format optimized for analytics, 10x cheaper
- **HDFS COLD**: Maximum compression, compliance storage, 20x cheaper

#### Metadata Storage Policy

**Hive Metastore** stores:
1. **Schema metadata**: Column names, types, nullability
2. **Partition metadata**: Date ranges, file locations
3. **Statistics**: Row counts, min/max values, distinct counts
4. **File locations**: HDFS paths for each partition

**Example Hive Table**:
```sql
CREATE EXTERNAL TABLE telemetry_warm (
    vehicle_id STRING,
    driver_id STRING,
    timestamp TIMESTAMP,
    speed DOUBLE,
    fuel_level DOUBLE,
    engine_temp DOUBLE
)
PARTITIONED BY (year INT, month INT, day INT)
STORED AS PARQUET
LOCATION '/warehouse/warm/telemetry/';
```

**Metadata Format**:
```json
{
  "table_name": "telemetry_warm",
  "format": "parquet",
  "partitions": [
    {
      "year": 2025,
      "month": 12,
      "day": 23,
      "location": "hdfs://namenode:9000/warehouse/warm/telemetry/year=2025/month=12/day=23/",
      "row_count": 864000,
      "size_mb": 103.68,
      "created_at": "2025-12-24T00:00:00Z"
    }
  ]
}
```

#### Archiving Workflow (Airflow DAGs)

**DAG 1: HOT â†’ WARM (Daily at midnight)**
```python
# airflow/dags/archiving_dag.py
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

dag = DAG(
    'hot_to_warm_archiving',
    schedule_interval='0 0 * * *',  # Daily at midnight
    start_date=datetime(2025, 1, 1)
)

def archive_to_warm():
    # 1. Query MongoDB for data older than 48 hours
    cutoff_time = datetime.now() - timedelta(hours=48)
    old_data = db.telemetry_events.find({'timestamp': {'$lt': cutoff_time}})
    
    # 2. Convert to DataFrame
    df = spark.createDataFrame(list(old_data))
    
    # 3. Write to HDFS as Parquet (partitioned by date)
    df.write.partitionBy('year', 'month', 'day').parquet(
        'hdfs://namenode:9000/warehouse/warm/telemetry/',
        mode='append',
        compression='snappy'
    )
    
    # 4. Update Hive metastore
    spark.sql("MSCK REPAIR TABLE telemetry_warm")
    
    # 5. Delete from MongoDB (free up space)
    db.telemetry_events.delete_many({'timestamp': {'$lt': cutoff_time}})

archive_task = PythonOperator(
    task_id='archive_to_warm',
    python_callable=archive_to_warm,
    dag=dag
)
```

---

## ðŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.11+
- 16GB RAM minimum
- 20GB free disk space

### Installation

```bash
# 1. Clone repository
git clone https://github.com/Sameed-Ahmad/fleet-analytics-pipeline.git
cd fleet-analytics-pipeline

# 2. Create Python virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start all Docker services
docker-compose up -d

# Wait 2-3 minutes for all services to initialize
```

### Run the Pipeline

```bash
# Start the complete data pipeline
./scripts/start_pipeline.sh

# The script will start:
# - MongoDB Kafka Consumer (writes to database)
# - Fleet Data Generator (creates events every 3 seconds)
# - Redis Cache Updater (refreshes KPIs every 60 seconds)

# Press Ctrl+C to stop all components
```

### Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Grafana Dashboard | http://localhost:3000 | admin / admin123 |
| Airflow UI | http://localhost:8080 | admin / admin123 |
| Spark Master UI | http://localhost:8081 | No auth |
| HDFS NameNode | http://localhost:9870 | No auth |

---

## ðŸ“ Complete Project Structure

```
fleet-analytics-pipeline/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ docker-compose.yml                  # All services definition
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”‚
â”œâ”€â”€ airflow/                           # Orchestration (Requirement #6)
â”‚   â””â”€â”€ dags/                          # 7 Airflow DAGs
â”‚       â”œâ”€â”€ data_generation_dag.py     # Continuous data generation
â”‚       â”œâ”€â”€ stream_processing_dag.py   # Monitor Kafka consumers
â”‚       â”œâ”€â”€ etl_staging_dag.py         # MongoDB â†’ Redis staging (60s)
â”‚       â”œâ”€â”€ archiving_dag.py           # HOT â†’ WARM archiving (daily)
â”‚       â”œâ”€â”€ cold_archiving_dag.py      # WARM â†’ COLD archiving (monthly)
â”‚       â”œâ”€â”€ analytics_dag.py           # Calculate KPIs (60s)
â”‚       â””â”€â”€ data_quality_dag.py        # Data validation (hourly)
â”‚
â”œâ”€â”€ config/                            # Configuration files
â”‚   â”œâ”€â”€ generator_config.json          # Data generator settings
â”‚   â”œâ”€â”€ kafka_config.json              # Kafka broker settings
â”‚   â””â”€â”€ spark_config.json              # Spark cluster config
â”‚
â”œâ”€â”€ data_generators/                   # Statistical Models (Req #3)
â”‚   â”œâ”€â”€ fleet_generator_kafka.py       # Main generator (Kafka producer)
â”‚   â”œâ”€â”€ models/                        # AI-based statistical models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ markov_route.py           # Markov Chain GPS routes
â”‚   â”‚   â”œâ”€â”€ gaussian_speed.py         # Gaussian speed distribution
â”‚   â”‚   â”œâ”€â”€ poisson_incidents.py      # Poisson incident generation
â”‚   â”‚   â”œâ”€â”€ ar1_telemetry.py          # AR(1) engine telemetry
â”‚   â”‚   â””â”€â”€ hmm_driver.py             # Hidden Markov Model behavior
â”‚   â””â”€â”€ utils/                         # Helper utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ data_faker.py             # Faker for dimension data
â”‚       â””â”€â”€ validators.py             # Data validation functions
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md                # Detailed architecture docs
â”‚   â”œâ”€â”€ SQL_QUERIES.md                 # Query examples & explanations
â”‚   â”œâ”€â”€ API_REFERENCE.md               # API documentation
â”‚   â””â”€â”€ TEAM_CONTRIBUTIONS.md          # Individual contributions
â”‚
â”œâ”€â”€ grafana/                           # Visualization (Requirement #6)
â”‚   â”œâ”€â”€ dashboards/                    # Dashboard JSON files
â”‚   â”‚   â””â”€â”€ fleet_dashboard.json      # Main 10-panel dashboard
â”‚   â””â”€â”€ provisioning/                  # Auto-provisioning configs
â”‚       â””â”€â”€ datasources/
â”‚           â”œâ”€â”€ mongodb.yml           # MongoDB datasource
â”‚           â””â”€â”€ redis.yml             # Redis datasource
â”‚
â”œâ”€â”€ hadoop/                            # Archive Storage (Req #4, #6)
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ format_namenode.sh        # Initialize HDFS
â”‚       â”œâ”€â”€ create_directories.sh     # Create /warehouse folders
â”‚       â””â”€â”€ hive_tables.sql           # Hive table definitions
â”‚
â”œâ”€â”€ kafka_services/                    # Message Queue (Requirement #6)
â”‚   â”œâ”€â”€ producers/                     # Kafka producers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ telemetry_producer.py     # Publish to topics
â”‚   â””â”€â”€ consumers/                     # Kafka consumers
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ mongodb_consumer.py        # Main consumer (continuous)
â”‚
â”œâ”€â”€ logs/                              # Log files (auto-created)
â”‚   â”œâ”€â”€ generator.log                  # Data generator logs
â”‚   â”œâ”€â”€ mongodb_consumer.log           # Consumer logs
â”‚   â””â”€â”€ redis_updater.log              # Cache updater logs
â”‚
â”œâ”€â”€ mongodb/                           # Hot Storage (Requirement #6)
â”‚   â”œâ”€â”€ init/                          # Initialization scripts
â”‚   â”‚   â””â”€â”€ init_dimensions.py        # Load 500 vehicles, 350 drivers
â”‚   â”œâ”€â”€ queries/                       # Query examples
â”‚   â”‚   â”œâ”€â”€ aggregations.js           # MongoDB aggregation queries
â”‚   â”‚   â””â”€â”€ geospatial.js             # Geospatial queries
â”‚   â”œâ”€â”€ schemas/                       # Schema definitions
â”‚   â”‚   â”œâ”€â”€ telemetry_schema.json     # Telemetry event schema
â”‚   â”‚   â””â”€â”€ delivery_schema.json      # Delivery schema
â”‚   â””â”€â”€ scripts/                       # Utility scripts
â”‚       â”œâ”€â”€ create_indexes.js         # Create all indexes
â”‚       â””â”€â”€ backup_collections.sh     # Backup script
â”‚
â”œâ”€â”€ redis_staging/                     # Cache/Staging (Req #5, #6)
â”‚   â”œâ”€â”€ etl_staging.py                 # ETL staging implementation
â”‚   â””â”€â”€ cache_manager.py               # Cache TTL management
â”‚
â”œâ”€â”€ scripts/                           # Utility scripts
â”‚   â”œâ”€â”€ start_pipeline.sh              # Main startup script
â”‚   â”œâ”€â”€ stop_pipeline.sh               # Graceful shutdown
â”‚   â”œâ”€â”€ mongodb_to_redis.py            # KPI calculator & cacher
â”‚   â”œâ”€â”€ auto_update_redis.sh           # Continuous Redis updater
â”‚   â”œâ”€â”€ check_redis.sh                 # Redis diagnostic tool
â”‚   â”œâ”€â”€ validate_data.py               # Data quality checks
â”‚   â””â”€â”€ performance_test.py            # Performance benchmarking
â”‚
â”œâ”€â”€ spark_jobs/                        # Analytics Engine (Req #6)
â”‚   â”œâ”€â”€ analytics/                     # Batch analytics
â”‚   â”‚   â”œâ”€â”€ kpi_calculator.py         # Calculate 10 KPIs
â”‚   â”‚   â””â”€â”€ olap_queries.py           # Complex SQL queries
â”‚   â”œâ”€â”€ config/                        # Spark configuration
â”‚   â”‚   â””â”€â”€ spark_defaults.conf       # Spark settings
â”‚   â”œâ”€â”€ etl/                           # ETL jobs
â”‚   â”‚   â”œâ”€â”€ mongodb_to_redis.py       # Staging ETL
â”‚   â”‚   â””â”€â”€ data_cleaner.py           # Data cleaning
â”‚   â””â”€â”€ streaming/                     # Stream processing
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ consumer.py               # Spark Streaming consumer
â”‚       â”œâ”€â”€ aggregations.py           # Real-time aggregations
â”‚       â”œâ”€â”€ mongodb_writer.py         # Write to MongoDB
â”‚       â””â”€â”€ utils/
â”‚           â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ spark_logs/                        # Spark execution logs
â”‚   â”œâ”€â”€ spark-master.out               # Master node logs
â”‚   â””â”€â”€ spark-worker.out               # Worker node logs
â”‚
â”œâ”€â”€ sql_queries/                       # SQL Documentation (Req #3)
â”‚   â”œâ”€â”€ showcase_queries.sql           # 5 featured JOIN queries
â”‚   â”œâ”€â”€ olap_examples.sql              # OLAP query examples
â”‚   â””â”€â”€ performance_queries.sql        # Optimized queries
â”‚
â”œâ”€â”€ tests/                             # Testing suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_generators.py             # Unit tests for generators
â”‚   â”œâ”€â”€ test_kafka.py                  # Kafka integration tests
â”‚   â”œâ”€â”€ test_mongodb.py                # MongoDB tests
â”‚   â”œâ”€â”€ test_redis.py                  # Redis tests
â”‚   â”œâ”€â”€ test_spark_jobs.py             # Spark job tests
â”‚   â”œâ”€â”€ test_integration.py            # End-to-end tests
â”‚   â””â”€â”€ test_performance.py            # Performance benchmarks
â”‚
â”œâ”€â”€ venv/                              # Python virtual environment
â”‚
â””â”€â”€ .env                               # Environment variables
```

---

## ðŸ”„ Airflow DAGs (7 Total) - Requirement #6

### DAG 1: Data Generation DAG
**File**: `airflow/dags/data_generation_dag.py`
```python
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime

dag = DAG(
    'data_generation_dag',
    schedule_interval='@continuous',
    start_date=datetime(2025, 1, 1),
    catchup=False
)

generate_task = BashOperator(
    task_id='generate_fleet_data',
    bash_command='python /app/data_generators/fleet_generator_kafka.py --interval 3',
    dag=dag
)
```

### DAG 2: Stream Processing DAG
**File**: `airflow/dags/stream_processing_dag.py`
```python
dag = DAG(
    'stream_processing_dag',
    schedule_interval='@continuous',
    start_date=datetime(2025, 1, 1)
)

monitor_task = PythonOperator(
    task_id='monitor_kafka_consumer',
    python_callable=monitor_consumer_health,
    dag=dag
)
```

### DAG 3: ETL Staging DAG
**File**: `airflow/dags/etl_staging_dag.py`
```python
dag = DAG(
    'etl_staging_dag',
    schedule_interval='*/1 * * * *',  # Every 60 seconds
    start_date=datetime(2025, 1, 1)
)

etl_task = BashOperator(
    task_id='mongodb_to_redis_staging',
    bash_command='python /app/scripts/mongodb_to_redis.py',
    dag=dag
)
```

### DAG 4: Hot to Warm Archiving DAG
**File**: `airflow/dags/archiving_dag.py`
```python
dag = DAG(
    'archiving_dag',
    schedule_interval='0 0 * * *',  # Daily at midnight
    start_date=datetime(2025, 1, 1)
)

archive_task = PythonOperator(
    task_id='archive_to_warm',
    python_callable=archive_to_warm_tier,
    dag=dag
)
```

### DAG 5: Warm to Cold Archiving DAG
**File**: `airflow/dags/cold_archiving_dag.py`
```python
dag = DAG(
    'cold_archiving_dag',
    schedule_interval='0 0 1 * *',  # Monthly on 1st
    start_date=datetime(2025, 1, 1)
)

cold_archive_task = PythonOperator(
    task_id='archive_to_cold',
    python_callable=archive_to_cold_tier,
    dag=dag
)
```

### DAG 6: Analytics DAG
**File**: `airflow/dags/analytics_dag.py`
```python
dag = DAG(
    'analytics_dag',
    schedule_interval='*/1 * * * *',  # Every 60 seconds
    start_date=datetime(2025, 1, 1)
)

kpi_task = BashOperator(
    task_id='calculate_kpis',
    bash_command='spark-submit /app/spark_jobs/analytics/kpi_calculator.py',
    dag=dag
)
```

### DAG 7: Data Quality DAG
**File**: `airflow/dags/data_quality_dag.py`
```python
dag = DAG(
    'data_quality_dag',
    schedule_interval='0 * * * *',  # Hourly
    start_date=datetime(2025, 1, 1)
)

quality_task = PythonOperator(
    task_id='validate_data_quality',
    python_callable=run_quality_checks,
    dag=dag
)
```

---

## ðŸ§ª Testing & Validation

### Verify Data Flow

```bash
# Check MongoDB data
docker exec fleet-mongodb mongosh -u admin -p admin123 --eval "
  use fleet_analytics;
  db.telemetry_events.countDocuments()
"

# Check Redis cache
docker exec fleet-redis redis-cli -a redis123 KEYS "cache:*"

# Monitor logs
tail -f logs/generator.log
tail -f logs/mongodb_consumer.log
tail -f logs/redis_updater.log

# Check Kafka topics
docker exec fleet-kafka kafka-topics.sh --list --bootstrap-server localhost:9092

# Check HDFS
docker exec namenode hdfs dfs -ls /warehouse/warm/telemetry/
```

### Data Quality Checks

- âœ… Telemetry events have valid GPS coordinates (24-38Â°N, 60-78Â°E for Pakistan)
- âœ… Speed values within realistic range (0-120 km/h)
- âœ… Fuel levels decrease over time for active vehicles
- âœ… No duplicate vehicle IDs in fleet (500 unique)
- âœ… All incidents linked to valid vehicles/drivers
- âœ… Timestamps in ascending order
- âœ… All foreign keys valid (referential integrity)

---

## ðŸ“Š Dashboard & KPIs

### Grafana Dashboard (10 Panels)

| Panel # | KPI | Redis Key | Update Frequency |
|---------|-----|-----------|------------------|
| 1 | Total Vehicles | `cache:kpi:total_vehicles` | Static (500) |
| 2 | Average Speed | `cache:kpi:avg_speed_kmh` | 60 seconds |
| 3 | Drivers On Route | `cache:kpi:drivers_on_route` | 60 seconds |
| 4 | Total Incidents | `cache:kpi:total_incidents` | 60 seconds |
| 5 | Average Fuel Level | `cache:kpi:avg_fuel_level_percent` | 60 seconds |
| 6 | Delivery Success Rate | `cache:kpi:delivery_success_rate_percent` | 60 seconds |
| 7 | Average Driver Rating | `cache:kpi:avg_driver_rating` | 60 seconds |
| 8 | Total Deliveries | `cache:kpi:total_deliveries` | 60 seconds |
| 9 | Vehicles by Type | `cache:mongo:vehicles_by_type` | Static |
| 10 | Fleet Utilization | `cache:kpi:fleet_utilization_percent` | 60 seconds |

**Access**: http://localhost:3000 (admin/admin123)

---

## ðŸ›‘ Stopping the Pipeline

```bash
# Graceful shutdown (recommended)
# Press Ctrl+C in the terminal running start_pipeline.sh

# Or use stop script
./scripts/stop_pipeline.sh

# Stop Docker services
docker-compose down

# Stop and remove volumes (clean slate)
docker-compose down -v
```